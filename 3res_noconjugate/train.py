import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from torch.utils.data import DataLoader, TensorDataset
import warnings

warnings.filterwarnings('ignore')  # å±è”½æ— å…³è­¦å‘Š


# -------------------------- 1. å·¥å…·å‡½æ•°ï¼šè®¾ç½®ä¸­æ–‡å­—ä½“ --------------------------
def set_chinese_font():
    try:
        fm.fontManager.addfont('C:/Windows/Fonts/simhei.ttf')
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    except:
        try:
            fm.fontManager.addfont('/Library/Fonts/Songti.ttc')
            plt.rcParams['font.sans-serif'] = ['Songti SC', 'DejaVu Sans']
        except:
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS']
    plt.rcParams['axes.unicode_minus'] = False


set_chinese_font()

# -------------------------- 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†ï¼ˆå·²ç§»é™¤æ•°æ®æ¸…æ´—ï¼‰ --------------------------
# è¯»å–Excelæ•°æ®ï¼ˆéœ€ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼‰
try:
    excel_file = pd.ExcelFile('../S21æ‰¹é‡æ‹Ÿåˆæ±‡æ€»ç»“æœ(å«ç›´æµé¡¹å’Œæ¯”ä¾‹é¡¹).xlsx')
    df = excel_file.parse('False')  # è¯»å–éå…±è½­æç‚¹çš„Falseå·¥ä½œè¡¨
    print(f"æˆåŠŸè¯»å–åŸå§‹æ•°æ®ï¼šå…±{len(df)}æ¡æ ·æœ¬ï¼Œ{len(df.columns)}åˆ—ç‰¹å¾")
    # æ ¡éªŒå¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
    required_cols_base = ['Cs', 'Rd', 'Rs', 'Pole1_Real', 'Pole2_Real', 'Pole3_Real', 'Residue1_Real', 'Residue2_Real', 'Residue3_Real']
    missing_cols = [col for col in required_cols_base if col not in df.columns]
    if missing_cols:
        raise ValueError(f"Excelç¼ºå°‘å¿…è¦åˆ—ï¼š{', '.join(missing_cols)}")
except FileNotFoundError:
    raise FileNotFoundError("Excelæ–‡ä»¶æœªæ‰¾åˆ°ï¼è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
except Exception as e:
    raise Exception(f"è¯»å–æ•°æ®å¤±è´¥ï¼š{str(e)}")

# -------------------------- ç‰¹å¾å·¥ç¨‹ï¼š9ç‰¹å¾ï¼ˆ6åŸºç¡€+3äº¤å‰ï¼‰ --------------------------
df_feat = df.copy()  # ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®æ„å»ºç‰¹å¾ï¼Œä¸åšæ¸…æ´—

# æ–°å¢äº¤å‰ç‰¹å¾
df_feat['Cs_Pole2'] = df_feat['Cs'] * df_feat['Pole2_Real']
df_feat['Pole2_Pole3'] = df_feat['Pole2_Real'] / (df_feat['Pole3_Real'] + 1e-6)  # é¿å…é™¤é›¶
df_feat['Cs_Pole3'] = df_feat['Cs'] * df_feat['Pole3_Real']

# å®šä¹‰è¾“å…¥ç‰¹å¾ï¼ˆ9ä¸ªï¼‰
input_feats = [
    'Cs', 'Rd', 'Rs', 'Pole1_Real', 'Pole2_Real', 'Pole3_Real',  # 6åŸºç¡€ç‰¹å¾
    'Cs_Pole2', 'Pole2_Pole3', 'Cs_Pole3'  # 3äº¤å‰ç‰¹å¾
]
print(f"\nç‰¹å¾å·¥ç¨‹å®Œæˆï¼šè¾“å…¥ç‰¹å¾ä»6ä¸ªæ‰©å±•ä¸º9ä¸ªï¼ˆ6åŸºç¡€+3äº¤å‰ï¼‰ï¼Œä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆæœªæ¸…æ´—ï¼‰")

# æå–è¾“å…¥å’Œè¾“å‡º
X = df_feat[input_feats].values
y = df_feat[['Residue1_Real', 'Residue2_Real', 'Residue3_Real']].values
print(f"é¢„æµ‹ç›®æ ‡ï¼š3ä¸ªåŸå§‹ç•™æ•°ï¼ˆæœªæ‰§è¡Œå¼‚å¸¸å€¼æ¸…æ´—ï¼‰")

# åˆ’åˆ†è®­ç»ƒé›†ï¼ˆ80%ï¼‰å’Œæµ‹è¯•é›†ï¼ˆ20%ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=17, shuffle=True
)
print(f"\næ•°æ®é›†åˆ’åˆ†å®Œæˆï¼šè®­ç»ƒé›†{len(X_train)}æ¡ï¼Œæµ‹è¯•é›†{len(X_test)}æ¡")
print(f"è¾“å…¥ç‰¹å¾ç»´åº¦ï¼š{X_train.shape[1]}ï¼ˆ9ç‰¹å¾ï¼‰ï¼Œè¾“å‡ºç•™æ•°ç»´åº¦ï¼š{y_train.shape[1]}ï¼ˆ3ç•™æ•°ï¼‰")

# åˆ†æç•™æ•°åˆ†å¸ƒï¼ˆåŸå§‹æ•°æ®ï¼Œå¯èƒ½å«æç«¯å€¼ï¼‰
y_train_min = np.min(y_train)
y_train_max = np.max(y_train)
print(f"\nåŸå§‹ç•™æ•°åˆ†å¸ƒï¼šè®­ç»ƒé›†ç•™æ•°æœ€å°å€¼={y_train_min:.6f}ï¼Œæœ€å¤§å€¼={y_train_max:.6f}ï¼ˆå¯èƒ½å«æç«¯å€¼ï¼‰")

# -------------------------- æ•°æ®æ ‡å‡†åŒ–ï¼ˆ9è¾“å…¥+åŸå§‹ç•™æ•°ï¼‰ --------------------------
# è¾“å…¥æ ‡å‡†åŒ–ï¼ˆ9ç‰¹å¾ï¼‰
scaler_X = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

# è¾“å‡ºæ ‡å‡†åŒ–ï¼ˆåŸå§‹ç•™æ•°ï¼‰
scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train)
y_test_scaled = scaler_y.transform(y_test)
print("\næ•°æ®æ ‡å‡†åŒ–å®Œæˆï¼š")
print(f" - è¾“å…¥ï¼ˆ9ç‰¹å¾ï¼‰ï¼šå‡å€¼={scaler_X.mean_.round(4)[:3]}...ï¼Œæ ‡å‡†å·®={scaler_X.scale_.round(4)[:3]}...")
print(f" - è¾“å‡ºï¼ˆåŸå§‹ç•™æ•°ï¼‰ï¼šå‡å€¼={scaler_y.mean_.round(4)}ï¼Œæ ‡å‡†å·®={scaler_y.scale_.round(4)}")

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_dataset = TensorDataset(torch.FloatTensor(X_train_scaled), torch.FloatTensor(y_train_scaled))
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=False)
X_test_tensor = torch.FloatTensor(X_test_scaled)
y_test_tensor = torch.FloatTensor(y_test_scaled)
print(f"\næ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼šæ‰¹é‡å¤§å°{train_loader.batch_size}ï¼Œè®­ç»ƒæ‰¹æ¬¡æ•°{len(train_loader)}")


# -------------------------- 3. ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆLeakyReLUæ¿€æ´»ï¼‰ --------------------------
class ResiduePredictor(nn.Module):
    def __init__(self):
        super(ResiduePredictor, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(9, 256),  # 9è¾“å…¥ç‰¹å¾
            nn.BatchNorm1d(256),
            nn.LeakyReLU(0.01),  # ç¼“è§£æ­»äº¡ç¥ç»å…ƒ
            nn.Dropout(0.1),

            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.LeakyReLU(0.01),

            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.LeakyReLU(0.01),
            nn.Dropout(0.1),

            nn.Linear(64, 32),
            nn.LeakyReLU(0.01),

            nn.Linear(32, 3)  # 3è¾“å‡ºç•™æ•°
        )

    def forward(self, x):
        return self.model(x)


# -------------------------- 4. æ¨¡å‹åˆå§‹åŒ–ä¸è®­ç»ƒé…ç½® --------------------------
model = ResiduePredictor()
criterion = nn.MSELoss()  # ä½¿ç”¨MSEæŸå¤±
optimizer = optim.Adam(
    model.parameters(),
    lr=0.001,
    weight_decay=1e-5  # L2æ­£åˆ™åŒ–
)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=10, verbose=True
)

# è®­ç»ƒè¶…å‚æ•°
epochs = 1000
best_test_loss = float('inf')
train_losses = []
test_losses = []
print(f"\nè®­ç»ƒé…ç½®å®Œæˆï¼š")
print(f" - æ€»è½®æ¬¡ï¼š{epochs}ï¼Œåˆå§‹å­¦ä¹ ç‡ï¼š0.001ï¼Œä¼˜åŒ–å™¨ï¼šAdamï¼ˆL2æ­£åˆ™åŒ–1e-5ï¼‰")
print(f" - å…³é”®è®¾ç½®ï¼š9ç‰¹å¾ + LeakyReLUæ¿€æ´»ï¼ˆæœªæ‰§è¡Œå¼‚å¸¸å€¼æ¸…æ´—ï¼‰")

# -------------------------- 5. æ¨¡å‹è®­ç»ƒ --------------------------
print("\n" + "=" * 80)
print("å¼€å§‹è®­ç»ƒï¼ˆæ¯10è½®æ‰“å°æ—¥å¿—ï¼Œæµ‹è¯•æŸå¤±ä¸‹é™æ—¶ä¿å­˜æœ€ä½³æ¨¡å‹ï¼‰")
print("=" * 80)

for epoch in range(epochs):
    # è®­ç»ƒé˜¶æ®µ
    model.train()
    train_loss = 0.0

    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * batch_X.size(0)

    avg_train_loss = train_loss / len(train_loader.dataset)
    train_losses.append(avg_train_loss)

    # éªŒè¯é˜¶æ®µ
    model.eval()
    with torch.no_grad():
        outputs = model(X_test_tensor)
        test_loss = criterion(outputs, y_test_tensor).item()
        test_losses.append(test_loss)

        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆæ ‡è®°æœªæ¸…æ´—ï¼‰
        if test_loss < best_test_loss:
            best_test_loss = test_loss
            torch.save(model.state_dict(), 'best_residue_predictor_9feat_leakyrelu.pth')
            best_model_info = {
                'epoch': epoch + 1,
                'train_loss': avg_train_loss,
                'test_loss': test_loss,
                'input_feats': input_feats,
                'scaler_X': {'mean': scaler_X.mean_.tolist(), 'std': scaler_X.scale_.tolist()},
                'scaler_y': {'mean': scaler_y.mean_.tolist(), 'std': scaler_y.scale_.tolist()},
                'batch_size': train_loader.batch_size,
                'lr': optimizer.param_groups[0]['lr'],
                'cleaned': False  # æ ‡è®°æœªæ‰§è¡Œå¼‚å¸¸å€¼æ¸…æ´—
            }
            np.save('best_residue_info_9feat_leakyrelu.npy', best_model_info)
            print(f"Epoch {epoch + 1:4d}: æµ‹è¯•æŸå¤±{test_loss:.6f}ï¼ˆå†å²æœ€ä½³ï¼‰â†’ ä¿å­˜æ¨¡å‹")

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(test_loss)

    # æ¯10è½®æ‰“å°æ—¥å¿—
    if (epoch + 1) % 10 == 0:
        current_lr = optimizer.param_groups[0]['lr']
        print(
            f"Epoch [{epoch + 1:4d}/{epochs}] | è®­ç»ƒæŸå¤±: {avg_train_loss:.6f} | æµ‹è¯•æŸå¤±: {test_loss:.6f} | å­¦ä¹ ç‡: {current_lr:.6f}")

# -------------------------- 6. åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¯„ä¼° --------------------------
print("\n" + "=" * 80)
print("è®­ç»ƒç»“æŸï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹è¯„ä¼°ï¼ˆ9ç‰¹å¾+LeakyReLUï¼Œæœªæ¸…æ´—ï¼‰")
print("=" * 80)

try:
    model.load_state_dict(torch.load('best_residue_predictor_9feat_leakyrelu.pth'))
    best_info = np.load('best_residue_info_9feat_leakyrelu.npy', allow_pickle=True).item()
    print(f"âœ… æˆåŠŸåŠ è½½æœ€ä½³æ¨¡å‹ï¼š")
    print(f"   - å¯¹åº”è½®æ¬¡ï¼šEpoch {best_info['epoch']}ï¼Œæœ€ä½³æµ‹è¯•æŸå¤±ï¼š{best_info['test_loss']:.6f}")
    print(f"   - è¾“å…¥ç‰¹å¾æ•°ï¼š9ï¼ˆ{best_info['input_feats']}ï¼‰ï¼Œæœªæ‰§è¡Œå¼‚å¸¸å€¼æ¸…æ´—")
except FileNotFoundError:
    print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶ï¼Œä½¿ç”¨æœ€åä¸€è½®æ¨¡å‹è¯„ä¼°")
except Exception as e:
    print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{str(e)}ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹è¯„ä¼°")
model.eval()

# é¢„æµ‹ä¸åæ ‡å‡†åŒ–
with torch.no_grad():
    y_pred_scaled = model(X_test_tensor)
    y_pred = scaler_y.inverse_transform(y_pred_scaled.numpy())
    y_true = scaler_y.inverse_transform(y_test_scaled)

# -------------------------- 7. è®¡ç®—è¯„ä¼°æŒ‡æ ‡ --------------------------
mae_res1 = np.mean(np.abs(y_pred[:, 0] - y_true[:, 0]))
mae_res2 = np.mean(np.abs(y_pred[:, 1] - y_true[:, 1]))
mae_res3 = np.mean(np.abs(y_pred[:, 2] - y_true[:, 2]))
total_mae = (mae_res1 + mae_res2 + mae_res3) / 3

print(f"\nğŸ“Š åŸå§‹ç•™æ•°é¢„æµ‹è¯„ä¼°ç»“æœï¼ˆå¹³å‡ç»å¯¹è¯¯å·®MAEï¼‰ï¼š")
print(f"   - ç•™æ•°1ï¼š{mae_res1:.4f}")
print(f"   - ç•™æ•°2ï¼š{mae_res2:.4f}")
print(f"   - ç•™æ•°3ï¼š{mae_res3:.4f}")
print(f"   - æ€»å¹³å‡è¯¯å·®ï¼š{total_mae:.4f}")

# -------------------------- 8. å¯è§†åŒ–ç»“æœ --------------------------
# 8.1 æŸå¤±æ›²çº¿ï¼ˆæ ‡è®°æœªæ¸…æ´—ï¼‰
plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), train_losses, color='#2E86AB', linewidth=1.5, label='è®­ç»ƒæŸå¤±')
plt.plot(range(1, epochs + 1), test_losses, color='#A23B72', linewidth=1.5, label='æµ‹è¯•æŸå¤±')
if 'best_info' in locals():
    best_epoch = best_info['epoch']
    best_loss = best_info['test_loss']
    plt.scatter(best_epoch, best_loss, color='red', s=50, zorder=5, label=f'æœ€ä½³æ¨¡å‹ï¼ˆEpoch{best_epoch}ï¼‰')
plt.xlabel('è®­ç»ƒè½®æ¬¡ï¼ˆEpochï¼‰', fontsize=11)
plt.ylabel('æŸå¤±å€¼ï¼ˆMSEï¼‰', fontsize=11)
plt.title('9ç‰¹å¾â†’3ç•™æ•° è®­ç»ƒä¸æµ‹è¯•æŸå¤±æ›²çº¿ï¼ˆLeakyReLUï¼Œæœªæ¸…æ´—ï¼‰', fontsize=13, pad=15)
plt.legend(fontsize=10)
plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
plt.savefig('residue_loss_curve_9feat_leakyrelu.png', dpi=300, bbox_inches='tight')
plt.close()
print(f"\nğŸ“ˆ æŸå¤±æ›²çº¿å·²ä¿å­˜ä¸ºï¼šresidue_loss_curve_9feat_leakyrelu.png")

# 8.2 ç•™æ•°é¢„æµ‹å¯¹æ¯”å›¾
sample_num = min(50, len(y_true))
sample_indices = np.arange(sample_num)
fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)

for i, ax in enumerate(axes):
    ax.plot(sample_indices, y_true[sample_indices, i], color='#2E86AB', linewidth=2, label='çœŸå®ç•™æ•°ï¼ˆåŸå§‹ï¼‰')
    ax.plot(sample_indices, y_pred[sample_indices, i], color='#FF0000', linewidth=1.5, linestyle='--', label='é¢„æµ‹ç•™æ•°')
    ax.set_title(f'ç•™æ•°{i + 1}é¢„æµ‹å€¼ä¸çœŸå®å€¼å¯¹æ¯”ï¼ˆå‰{sample_num}ä¸ªæ ·æœ¬ï¼Œæœªæ¸…æ´—ï¼‰', fontsize=12, pad=12)
    ax.set_ylabel('ç•™æ•°å€¼', fontsize=10)
    ax.legend(fontsize=9)
    ax.grid(True, linestyle='--', alpha=0.3)

axes[-1].set_xlabel('æ ·æœ¬ç´¢å¼•', fontsize=10)
plt.tight_layout()
plt.savefig('residue_pred_comparison_9feat_leakyrelu.png', dpi=300, bbox_inches='tight')
plt.close()
print(f"ğŸ“Š ç•™æ•°å¯¹æ¯”å›¾å·²ä¿å­˜ä¸ºï¼šresidue_pred_comparison_9feat_leakyrelu.png")

# -------------------------- 9. æ‰“å°å‰10æ¡é¢„æµ‹ç»“æœæ˜ç»† --------------------------
print("\n" + "=" * 110)
print("å‰10æ¡æ ·æœ¬é¢„æµ‹ç»“æœæ˜ç»†ï¼ˆ9ç‰¹å¾+LeakyReLUï¼Œæœªæ¸…æ´—ï¼‰")
print("=" * 110)
print(
    f"{'æ ·æœ¬':<6} {'çœŸå®Res1':<12} {'é¢„æµ‹Res1':<12} {'çœŸå®Res2':<12} {'é¢„æµ‹Res2':<12} {'çœŸå®Res3':<12} {'é¢„æµ‹Res3':<12}")
print("-" * 110)
for i in range(min(10, len(y_pred))):
    print(
        f"{i:<6} {y_true[i, 0]:<12.4f} {y_pred[i, 0]:<12.4f} "
        f"{y_true[i, 1]:<12.4f} {y_pred[i, 1]:<12.4f} "
        f"{y_true[i, 2]:<12.4f} {y_pred[i, 2]:<12.4f}"
    )

# -------------------------- 10. è¾“å‡ºæ–‡ä»¶æ±‡æ€» --------------------------
print("\n" + "=" * 80)
print("æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜è‡³å½“å‰ç›®å½•ï¼ˆ9ç‰¹å¾+LeakyReLUï¼Œæœªæ¸…æ´—ï¼‰ï¼š")
print("1. best_residue_predictor_9feat_leakyrelu.pth â†’ æœ€ä½³æ¨¡å‹å‚æ•°")
print("2. best_residue_info_9feat_leakyrelu.npy â†’ è®­ç»ƒä¿¡æ¯ï¼ˆå«æœªæ¸…æ´—æ ‡è®°ï¼‰")
print("3. residue_loss_curve_9feat_leakyrelu.png â†’ æŸå¤±æ›²çº¿")
print("4. residue_pred_comparison_9feat_leakyrelu.png â†’ ç•™æ•°å¯¹æ¯”å›¾")
print("=" * 80)